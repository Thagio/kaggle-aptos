{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME  : 以下の関数は定義されたファイルの形式に依存するので、utilsに記載できない。\n",
    "def is_env_notebook():\n",
    "    \"\"\"Determine wheather is the environment Jupyter Notebook\"\"\"\n",
    "    if 'get_ipython' not in globals():\n",
    "        # Python shell\n",
    "        return False\n",
    "    env_name = get_ipython().__class__.__name__\n",
    "    if env_name == 'TerminalInteractiveShell':\n",
    "        # IPython shell\n",
    "        return False\n",
    "    # Jupyter Notebook\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from itertools import islice\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import warnings\n",
    "from typing import Dict\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import torch\n",
    "from torch import nn, cuda\n",
    "from torch.optim import Adam\n",
    "import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from IPython.core.debugger import Pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON_KAGGLE: bool = 'KAGGLE_WORKING_DIR' in os.environ\n",
    "\n",
    "if ON_KAGGLE:\n",
    "    from . import models\n",
    "    from .dataset import TrainDataset, TTADataset, get_ids, N_CLASSES, DATA_ROOT\n",
    "    from .transforms import train_transform, test_transform\n",
    "    from .utils import (\n",
    "        write_event, load_model, mean_df, ThreadingDataLoader as DataLoader,\n",
    "        ON_KAGGLE)\n",
    "else:\n",
    "    import models\n",
    "    from dataset import TrainDataset, TTADataset, get_ids, N_CLASSES, DATA_ROOT\n",
    "    from transforms import train_transform, test_transform\n",
    "    from utils import (\n",
    "        write_event, load_model, mean_df, ThreadingDataLoader as DataLoader,\n",
    "        ON_KAGGLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(*args):\n",
    "#def main():   \n",
    "   # print(\"do main\")\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    arg = parser.add_argument\n",
    "    \n",
    "    # TODO : \"--modeにGradCAMを追加\"\n",
    "    \n",
    "    arg('--mode', choices=['train', 'validate', 'predict_valid', 'predict_test'])\n",
    "    arg('--run_root')\n",
    "    arg('--model', default='resnet50')\n",
    "    arg('--loss',default=\"focalloss\")\n",
    "    arg('--pretrained', type=int, default=1)\n",
    "    arg('--batch-size', type=int, default=64)\n",
    "    arg('--step', type=int, default=1)\n",
    "    arg('--workers', type=int, default=4 if ON_KAGGLE else 4)\n",
    "    arg('--lr', type=float, default=1e-4)\n",
    "    arg('--patience', type=int, default=4)\n",
    "    arg('--clean', action='store_true')\n",
    "    arg('--n-epochs', type=int, default=100)\n",
    "    arg('--epoch-size', type=int)\n",
    "    arg('--tta', type=int, default=4)\n",
    "    arg('--use-sample', action='store_true', help='use a sample of the dataset')\n",
    "    arg('--debug', action='store_true')\n",
    "    arg('--limit', type=int)\n",
    "    arg('--fold', type=int, default=0)\n",
    "    arg('--regression',type=int,default=0)\n",
    "    arg('--finetuning',type=int,default=1)\n",
    "    # TODO : classificationかregressionかをオプションで追加できるようにする。\n",
    "  \n",
    "   # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    if is_env_notebook():       \n",
    "        args = parser.parse_args(args=args[0])\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    " #   from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    run_root = Path(args.run_root)\n",
    "    folds = pd.read_csv('folds.csv')\n",
    "    \n",
    "    train_root = DATA_ROOT / ('train_sample' if args.use_sample else 'train_images')\n",
    "    \n",
    "    if args.use_sample:\n",
    "        folds = folds[folds['Id'].isin(set(get_ids(train_root)))]\n",
    "    \n",
    "  #  Pdb().set_trace()\n",
    "    # -1 はleakデータ\n",
    "    train_fold = folds[folds['fold'] != args.fold]\n",
    "    leak_fold = folds[folds['fold'] == -1]\n",
    "    train_fold = pd.concat([train_fold,leak_fold])\n",
    "    \n",
    "    valid_fold = folds[folds['fold'] == args.fold]\n",
    "    \n",
    "    \n",
    "    if args.limit:\n",
    "        train_fold = train_fold[:args.limit]\n",
    "        valid_fold = valid_fold[:args.limit]\n",
    "        \n",
    "    \n",
    "    def make_loader(df: pd.DataFrame, image_transform,regression=args.regression,shuffle=False,balanced=True) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            TrainDataset(train_root, df, image_transform, debug=args.debug,regression=regression,balanced=balanced),\n",
    "            shuffle=shuffle,\n",
    "            batch_size=args.batch_size,\n",
    "            num_workers=args.workers,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ## TODO : regressionようにモデルを書き換え\n",
    "    \n",
    "    if args.regression:\n",
    "        criterion = nn.MSELoss()\n",
    "        # TODO : 回帰モデルへ変更\n",
    "        model = getattr(models, args.model)(\n",
    "            num_classes=1, pretrained=args.pretrained)\n",
    "\n",
    "    else:\n",
    "        # 分類モデル\n",
    "        criterion = FocalLoss()#nn.BCEWithLogitsLoss(reduction='none') \n",
    "        model = getattr(models, args.model)(\n",
    "            num_classes=N_CLASSES, pretrained=args.pretrained)\n",
    "\n",
    " #   Pdb().set_trace()\n",
    "    \n",
    "    use_cuda = cuda.is_available()\n",
    "    fresh_params = list(model.fresh_params())\n",
    "    all_params = list(model.parameters())\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    if args.mode == 'train':\n",
    "        if run_root.exists() and args.clean:\n",
    "            shutil.rmtree(run_root)\n",
    "        run_root.mkdir(exist_ok=True, parents=True)\n",
    "        (run_root / 'params.json').write_text(\n",
    "            json.dumps(vars(args), indent=4, sort_keys=True))\n",
    "\n",
    "        train_loader = make_loader(train_fold, train_transform,regression=args.regression,balanced=True)\n",
    "        valid_loader = make_loader(valid_fold, test_transform,regression=args.regression,balanced=False)\n",
    "        print(f'{len(train_loader.dataset):,} items in train, '\n",
    "              f'{len(valid_loader.dataset):,} in valid')\n",
    "\n",
    "        train_kwargs = dict(\n",
    "            args=args,\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            train_loader=train_loader,\n",
    "            valid_loader=valid_loader,\n",
    "            patience=args.patience,\n",
    "            init_optimizer=lambda params, lr: Adam(params, lr),\n",
    "            use_cuda=use_cuda,\n",
    "        )\n",
    "     #   from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        if args.pretrained:\n",
    "            if train(params=fresh_params, n_epochs=1, **train_kwargs):\n",
    "                train(params=all_params, **train_kwargs)\n",
    "        else:\n",
    "            train(params=all_params, **train_kwargs)\n",
    "            \n",
    "        # fine-tunig after balanced learning \n",
    "        if args.finetuning:\n",
    "            print(\"Start Fine-tuning\")\n",
    "            TUNING_EPOCH = 5\n",
    "            train_loader = make_loader(train_fold, train_transform,regression=args.regression,balanced=False)\n",
    "            # 学習率を小さくする\n",
    "            args.lr = args.lr / 5\n",
    "            train_kwargs[\"train_loader\"] = train_loader\n",
    "            train(params=all_params,n_epochs=args.n_epochs+TUNING_EPOCH,**train_kwargs,finetuning=args.finetuning)\n",
    "\n",
    "    elif args.mode == 'validate':\n",
    "        valid_loader = make_loader(valid_fold, test_transform)\n",
    "        load_model(model, run_root / 'model.pt')\n",
    "        validation(model, criterion, tqdm.tqdm(valid_loader, desc='Validation',valid_fold=valid_fold),\n",
    "                   use_cuda=use_cuda)\n",
    "\n",
    "    elif args.mode.startswith('predict'):\n",
    "        print(\"load model predict\")\n",
    "        load_model(model, run_root / 'best-model.pt')\n",
    "        predict_kwargs = dict(\n",
    "            batch_size=args.batch_size,\n",
    "            tta=args.tta,\n",
    "            use_cuda=use_cuda,\n",
    "            workers=args.workers,\n",
    "        )\n",
    "        if args.mode == 'predict_valid':\n",
    "            #predict(model, df=valid_fold, root=train_root,\n",
    "            #        out_path=run_root / 'val.h5',\n",
    "            #        **predict_kwargs)\n",
    "            \n",
    "            valid_loader = make_loader(valid_fold, test_transform,shuffle=False,balanced=False)\n",
    "            #model: nn.Module, criterion, valid_loader, use_cuda,valid_predict:bool=False\n",
    "            \n",
    "            # TODO : valid foldに予測結果をくっ付ける操作を追加\n",
    "            validation(model,criterion,valid_loader,use_cuda,valid_fold=valid_fold,valid_predict=True,save_path=run_root)\n",
    "                        \n",
    "        elif args.mode == 'predict_test':\n",
    "            test_root = DATA_ROOT / (\n",
    "                'test_sample' if args.use_sample else 'test_images')\n",
    "            ss = pd.read_csv(DATA_ROOT / 'sample_submission.csv')\n",
    "            if args.use_sample:\n",
    "                ss = ss[ss['id'].isin(set(get_ids(test_root)))]\n",
    "            if args.limit:\n",
    "                ss = ss[:args.limit]\n",
    "            predict(model, df=ss, root=test_root,\n",
    "                    out_path=run_root / 'test.h5',\n",
    "                    **predict_kwargs)\n",
    "\n",
    "\n",
    "def predict(model, root: Path, df: pd.DataFrame, out_path: Path,\n",
    "            batch_size: int, tta: int, workers: int, use_cuda: bool):\n",
    "    loader = DataLoader(\n",
    "        dataset=TTADataset(root, df, test_transform, tta=tta),\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=workers,\n",
    "    )\n",
    "    model.eval()\n",
    "    all_outputs, all_ids = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, ids in tqdm.tqdm(loader, desc='Predict'):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "            all_outputs.append(outputs.data.cpu().numpy())\n",
    "            all_ids.extend(ids)\n",
    "            \n",
    "    df = pd.DataFrame(\n",
    "        data=np.concatenate(all_outputs),\n",
    "        index=all_ids,\n",
    "        columns=map(str, range(N_CLASSES)))\n",
    "    df = mean_df(df)\n",
    "    df.to_hdf(out_path, 'prob', index_label='id')\n",
    "    print(f'Saved predictions to {out_path}')\n",
    "\n",
    "\n",
    "def train(args, model: nn.Module, criterion, *, params,\n",
    "          train_loader, valid_loader, init_optimizer, use_cuda,\n",
    "          n_epochs=None, patience=2, max_lr_changes=2,finetuning=False) -> bool:\n",
    "    \n",
    "    lr = args.lr\n",
    "    n_epochs = n_epochs or args.n_epochs\n",
    "    params = list(params)\n",
    "    optimizer = init_optimizer(params, lr)\n",
    "\n",
    "    run_root = Path(args.run_root)\n",
    "    model_path = run_root / 'model.pt'\n",
    "    best_model_path = run_root / 'best-model.pt'\n",
    "    \n",
    "    if model_path.exists():\n",
    "        state = load_model(model, model_path)\n",
    "        epoch = state['epoch']\n",
    "        step = state['step']\n",
    "        best_valid_loss = state['best_valid_loss']\n",
    "        \n",
    "    if best_model_path.exists() and finetuning:\n",
    "        state = load_model(model,best_model_path)\n",
    "        #epoch = 1\n",
    "        #step = 0\n",
    "        epoch = state['epoch']\n",
    "        step = state['step']\n",
    "        best_valid_loss = state['best_valid_loss']\n",
    "    \n",
    "    else:\n",
    "        epoch = 1\n",
    "        step = 0\n",
    "        best_valid_loss = float('inf')\n",
    "        \n",
    "    lr_changes = 0\n",
    "\n",
    "    save = lambda ep: torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'epoch': ep,\n",
    "        'step': step,\n",
    "        'best_valid_loss': best_valid_loss\n",
    "    }, str(model_path))\n",
    "\n",
    "    report_each = 10\n",
    "    log = run_root.joinpath('train.log').open('at', encoding='utf8')\n",
    "    valid_losses = []\n",
    "    lr_reset_epoch = epoch\n",
    "    for epoch in range(epoch, n_epochs + 1):\n",
    "        model.train()\n",
    "        tq = tqdm.tqdm(total=(args.epoch_size or\n",
    "                              len(train_loader) * args.batch_size))\n",
    "        tq.set_description(f'Epoch {epoch}, lr {lr}')\n",
    "        losses = []\n",
    "        tl = train_loader\n",
    "     #   from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "        if args.epoch_size:\n",
    "            tl = islice(tl, args.epoch_size // args.batch_size)\n",
    "            \n",
    "        try:\n",
    "            mean_loss = 0\n",
    "          #  Pdb().set_trace()\n",
    "            for i, (inputs, targets) in enumerate(tl):        \n",
    "                if use_cuda:\n",
    "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)#_reduce_loss(criterion(outputs, targets))\n",
    "                batch_size = inputs.size(0)\n",
    "                (batch_size * loss).backward()\n",
    "                if (i + 1) % args.step == 0:\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    step += 1\n",
    "                tq.update(batch_size)\n",
    "                losses.append(loss.item())\n",
    "                mean_loss = np.mean(losses[-report_each:])\n",
    "                tq.set_postfix(loss=f'{mean_loss:.3f}')\n",
    "                if i and i % report_each == 0:\n",
    "                    write_event(log, step, loss=mean_loss)\n",
    "          #  Pdb().set_trace()\n",
    "            write_event(log, step, loss=mean_loss)\n",
    "            tq.close()\n",
    "            save(epoch + 1)\n",
    "            valid_metrics = validation(model, criterion, valid_loader, use_cuda)\n",
    "            \n",
    "         #   Pdb().set_trace()\n",
    "            write_event(log, step, **valid_metrics)\n",
    "            \n",
    "            valid_loss = valid_metrics['valid_loss']\n",
    "            valid_losses.append(valid_loss)\n",
    "            \n",
    "        #    from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "            \n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                shutil.copy(str(model_path), str(best_model_path))\n",
    "            elif (patience and epoch - lr_reset_epoch > patience and\n",
    "                  min(valid_losses[-patience:]) > best_valid_loss):\n",
    "                # \"patience\" epochs without improvement\n",
    "                lr_changes +=1\n",
    "                if lr_changes > max_lr_changes:\n",
    "                    break\n",
    "                lr /= 5\n",
    "                print(f'lr updated to {lr}')\n",
    "                lr_reset_epoch = epoch\n",
    "                optimizer = init_optimizer(params, lr)\n",
    "        except KeyboardInterrupt:\n",
    "            tq.close()\n",
    "            print('Ctrl+C, saving snapshot')\n",
    "            save(epoch)\n",
    "            print('done.')\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def validation(\n",
    "        model: nn.Module, criterion, valid_loader, use_cuda,valid_fold=None,valid_predict:bool=False,save_path:Path=\"\"\n",
    "        ) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    all_losses, all_predictions, all_targets = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in valid_loader:\n",
    "            all_targets.append(targets.numpy().copy())\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "          #  all_losses.append(_reduce_loss(loss).item())\n",
    "            all_losses.append(loss.item())\n",
    "            predictions = torch.sigmoid(outputs)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    \n",
    "#    Pdb().set_trace()\n",
    "\n",
    "    def get_score(y_pred):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore', category=UndefinedMetricWarning)\n",
    "          #  return fbeta_score(\n",
    "          #      all_targets, y_pred, beta=2, average='samples')\n",
    "            return qk(y_pred,all_targets)\n",
    "\n",
    "    metrics = {}\n",
    "    #argsorted = all_predictions.argsort(axis=1)\n",
    "    \n",
    "   # Pdb().set_trace()\n",
    "    #for threshold in [0.05, 0.10, 0.15, 0.20]:\n",
    "    #    metrics[f'valid_f2_th_{threshold:.2f}'] = get_score(\n",
    "    #        binarize_prediction(all_predictions, threshold, argsorted))\n",
    "    #metrics = get_score(all_predictions) \n",
    "    \n",
    "    if valid_predict:\n",
    "     #   Pdb().set_trace()\n",
    "        # TOOD : 予測結果をpd データフレーム形式で保存\n",
    "        valid_fold[\"prediction\"] = all_predictions.argmax(axis=1)\n",
    "        valid_fold.to_csv(save_path / \"valid_prediction.csv\",index=False)\n",
    "        \n",
    "       # run_root = Path(args.run_root)\n",
    "        with open(save_path / \"best_score.txt\",mode=\"w\") as f:\n",
    "            f.write(\"best valid kapa : {score}\".format(score=get_score(all_predictions)))\n",
    "            f.write(\"best valid loss : {loss}\".format(loss=np.mean(all_losses)))\n",
    "    \n",
    " #   from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    metrics['valid_kapa'] = get_score(all_predictions)\n",
    "    metrics['valid_loss'] = np.mean(all_losses)\n",
    "    #print(' | '.join(f'{k} {v:.3f}' for k, v in sorted(\n",
    "    #    metrics.items(), key=lambda kv: -kv[1])))\n",
    "    print(metrics)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def visualization():\n",
    "    \"\"\"\n",
    "    GRAD-CAMによるNNが判断の根拠としている領域の可視化\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def binarize_prediction(probabilities, threshold: float, argsorted=None,\n",
    "                        min_labels=1, max_labels=10):\n",
    "    \"\"\" Return matrix of 0/1 predictions, same shape as probabilities.\n",
    "    \"\"\"\n",
    "    assert probabilities.shape[1] == N_CLASSES\n",
    "    if argsorted is None:\n",
    "        argsorted = probabilities.argsort(axis=1)\n",
    "    max_mask = _make_mask(argsorted, max_labels)\n",
    "    min_mask = _make_mask(argsorted, min_labels)\n",
    "    prob_mask = probabilities > threshold\n",
    "    return (max_mask & prob_mask) | min_mask\n",
    "\n",
    "\n",
    "def _make_mask(argsorted, top_n: int):\n",
    "    mask = np.zeros_like(argsorted, dtype=np.uint8)\n",
    "    col_indices = argsorted[:, -top_n:].reshape(-1)\n",
    "    row_indices = [i // top_n for i in range(len(col_indices))]\n",
    "    mask[row_indices, col_indices] = 1\n",
    "    return mask\n",
    "\n",
    "\n",
    "def _reduce_loss(loss):\n",
    "    return loss.sum() / loss.shape[0]\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "        target = target.float()\n",
    "        max_val = (-logit).clamp(min=0)\n",
    "        loss = logit - logit * target + max_val + \\\n",
    "               ((-max_val).exp() + (-logit - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-logit * (target * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        if len(loss.size())==2:\n",
    "            loss = loss.sum(dim=1)\n",
    "        return loss.mean()\n",
    "    \n",
    "def qk(y_pred, y):\n",
    "   ## Pdb().set_trace()\n",
    "    #y_pred = torch.from_numpy(y_pred)\n",
    "    y_pred = np.argmax(y_pred,axis=1)\n",
    "    y = np.argmax(y,axis=1)\n",
    "    #y = torch.argmax(y,dim=1)\n",
    "  #  Pdb().set_trace()\n",
    "    return cohen_kappa_score(y_pred, y, weights='quadratic')\n",
    "    #return torch.tensor(cohen_kappa_score(torch.round(y_pred), y, weights='quadratic'), device='cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__' and ON_KAGGLE:\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Epoch 1, lr 0.0001:   0%|          | 0/6784 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6,740 items in train, 841 in valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, lr 0.0001:  35%|███▍      | 2368/6784 [00:45<01:22, 53.84it/s, loss=0.667]"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and  not(ON_KAGGLE):\n",
    "    import gc\n",
    "    \n",
    "    ###########################################################\n",
    "    # FOLDを修正\n",
    "    \n",
    "    folds = [0,1,2,3]\n",
    "             #1,2,3]#[0,1,\n",
    "             \n",
    "  #  N_EPOCH = 25\n",
    "  #  model_name = \"02_brightness_cotrast\"\n",
    "    #model_name = \"HueSaturationValue\"\n",
    "    #model_name = \"RandomGamma\"\n",
    "  #  model_name = \"regression\"\n",
    "  #  model_name = \"RandomSizeCrop_validation\"\n",
    "  #  model_name = \"CentorCrop_Oneof\"\n",
    "    #model_name = \"CentorCrop_Rotation_Val\"\n",
    "  #  model_name = \"Rockman_aug_nonCircleCrop\"\n",
    "   # model_name = \"Rockman_aug_CircleCrop\"\n",
    "  #  model_name = \"10_test\"\n",
    "#    model_name = \"11_No_Crop_balanced_finetuning\"\n",
    "  #  model_name = \"12_add_11_scale_and_gamma\"\n",
    "   # model_name = \"13_nodup\"\n",
    "    model_name = \"14_nodup_refine\"\n",
    "    \n",
    "    N_EPOCH = 10\n",
    "    # limit変更\n",
    "    \n",
    "    for fold in folds:\n",
    "        # 学習\n",
    "        # jupyter-notebookの場合、ここで引数を選択しないといけない。\n",
    "        train_args = [\"--mode\",\"train\",\n",
    "                   \"--run_root\",\"{model_name}_{fold}\".format(model_name=model_name,fold=fold),\n",
    "               #    \"--limit\",\"100\", # TODO : 適宜変更\n",
    "                    \"--fold\",\"{fold}\".format(fold=fold),\n",
    "                   \"--n-epochs\",\"{epoch}\".format(epoch=N_EPOCH),\n",
    "                   '--workers',\"16\",\n",
    "                      '--patience',\"2\",\n",
    "                      \"--finetuning\",\"1\"\n",
    "                    # \"--regression\",\"1\"\n",
    "                     ]\n",
    "        \n",
    "        main(train_args)\n",
    "        \n",
    "        # validation\n",
    "        val_args = [\"--mode\",\"predict_valid\",\n",
    "               \"--run_root\",\"{model_name}_{fold}\".format(model_name=model_name,fold=fold),\n",
    "             #  \"--limit\",\"100\"\n",
    "                   ]\n",
    "        main(val_args)\n",
    "        \n",
    "        gc.collect()\n",
    "    #    break\n",
    "        \n",
    "        #print(N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Predict:   0%|          | 0/121 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 8, step 302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predict: 100%|██████████| 121/121 [02:13<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to CentorCrop_Rotation_Val/CentorCrop_Rotation_Val_0/test.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and not(ON_KAGGLE):\n",
    "    \n",
    "    model_name = \"CentorCrop_Rotation_Val/CentorCrop_Rotation_Val_0\"\n",
    "    # jupyter-notebookの場合、ここで引数を選択しないといけない。\n",
    "    arg_list = [\"--mode\",\"predict_test\",\n",
    "               \"--run_root\",model_name,\n",
    "           #    \"--limit\",\"100\",\n",
    "                \"--tta\",\"4\"\n",
    "               ]\n",
    "    main(arg_list)\n",
    "    #print(N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from epoch 10, step 18\n",
      "{'valid_kapa': 0.7760782692572689, 'valid_loss': 0.41338345408439636}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and not(ON_KAGGLE):\n",
    "    # jupyter-notebookの場合、ここで引数を選択しないといけない。\n",
    "    model_name = model_name + \"_0\"\n",
    "    \n",
    "    arg_list = [\"--mode\",\"predict_valid\",\n",
    "               \"--run_root\",model_name,\n",
    "               \"--limit\",\"100\"]\n",
    "    \n",
    "    main(arg_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
